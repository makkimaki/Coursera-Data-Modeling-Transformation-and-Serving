{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations with Apache Spark\n",
    "\n",
    "In this lab, you will perform transformations on the `classicmodels` database with Apache Spark. You will first practice the basics and then use `PySpark` to create a star schema model similar to the one done in the Week 1 assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Environment Setup](#2)\n",
    "- [ 3 - Apache Spark 101](#3)\n",
    "  - [ 3.1 - Spark Classes](#3.1)\n",
    "  - [ 3.2 - Spark DataFrame](#3.2)\n",
    "  - [ 3.3 - Spark SQL](#3.3)\n",
    "  - [ 3.4 - UDFs and Data Types](#3.4)\n",
    "- [ 4 - Data Modeling with Spark](#4)\n",
    "  - [ 4.1 - Read the Tables](#4.1)\n",
    "  - [ 4.2 - Star Schema](#4.2)\n",
    "  - [ 4.3 - Customers Dimension](#4.3)\n",
    "  - [ 4.4 - Products Dimension](#4.4)\n",
    "  - [ 4.5 - Offices Dimension](#4.5)\n",
    "  - [ 4.6 - Employees Dimension](#4.6)\n",
    "  - [ 4.7 - Date Dimension](#4.7)\n",
    "  - [ 4.8 - Fact Table](#4.8)\n",
    "- [ 5 - Upload Files for Grading](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "Apache Spark is an open-source unified analytics engine for large-scale data processing, it allows you to perform Data Engineering, Data Science, and Machine Learning jobs on single-node machines or clusters. In courses 1 and 3, you have seen some examples with AWS Glue jobs, a serverless service that allows you to run Spark jobs without setting up cloud resources. In this assignment, you are provided with a Spark cluster deployed using Amazon EMR. This service comes with a Studio and Workspace functions allowing you to run Spark jobs from this notebook directly.\n",
    "\n",
    "You will recreate the Star Schema data model from the Week 1 assignment using PySpark, the Python API for Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Environment Setup\n",
    "\n",
    "The `classicmodels` database is stored in an RDS instance running a Postgres engine, you will need to configure the connection to read the source data and then store the generated data models. Thankfully, the Studio functionality of Amazon EMR provides you with the necessary classes ready to use, but you will need to add a configuration to allow the environment to connect to a Postgres database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Run the following cell, this will point the Spark cluster to a JAR file with the necessary code to connect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:41:28.234819Z",
     "iopub.status.busy": "2025-03-17T09:41:28.234520Z",
     "iopub.status.idle": "2025-03-17T09:41:28.251258Z",
     "shell.execute_reply": "2025-03-17T09:41:28.250708Z",
     "shell.execute_reply.started": "2025-03-17T09:41:28.234791Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv', 'spark.jars.packages': 'org.postgresql:postgresql:42.2.5'}, 'proxyUser': 'assumed-role_voclabs_user3927559_hobcksovfbbd', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "        \"spark.jars.packages\": \"org.postgresql:postgresql:42.2.5\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Go to **CloudFormation** in the AWS console. You will see the stack with an alphanumeric ID. Click on it and search for the **Outputs** tab. You will see the key `PostgresEndpoint`, copy the corresponding **Value** (highlight and copy it as text, not as a link). Replace the placeholder `<RDS-ENDPOINT>` in the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:41:29.981509Z",
     "iopub.status.busy": "2025-03-17T09:41:29.981234Z",
     "iopub.status.idle": "2025-03-17T09:42:09.669256Z",
     "shell.execute_reply": "2025-03-17T09:42:09.668691Z",
     "shell.execute_reply.started": "2025-03-17T09:41:29.981475Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc7409ea1bb4a75a95807b090c852a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1742199454971_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-158.ec2.internal:20888/proxy/application_1742199454971_0003/\" class=\"emr-proxy-link j-1WO1NL7CO5FB9 application_1742199454971_0003\" emr-resource=\"j-1WO1NL7CO5FB9\n\" application-id=\"application_1742199454971_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-96.ec2.internal:8042/node/containerlogs/container_1742199454971_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDS_ENDPOINT = \"de-c4w3a1-rds.cbgo6q4iap6p.us-east-1.rds.amazonaws.com\"\n",
    "jdbc_url = f\"jdbc:postgresql://{RDS_ENDPOINT}:5432/postgres\"  # For PostgreSQL\n",
    "\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgresuser\",\n",
    "    \"password\": \"adminpwrd\",    \n",
    "    \"driver\": \"org.postgresql.Driver\"  # For PostgreSQL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous cell you should wait while the Spark application starts. After it finishes, you should see a message like this:\n",
    "```bash\n",
    "SparkSession available as 'spark'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Now use the `spark` object that is available to read from the database using Java Database Connectivity (JDBC), you will provide the JDBC url and the connection properties, and point to a table with their corresponding schema. In this case, you will call `information_schema.tables` to get the available tables; then you will select the schema and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:40.105249Z",
     "iopub.status.busy": "2025-03-17T09:43:40.105031Z",
     "iopub.status.idle": "2025-03-17T09:43:49.392296Z",
     "shell.execute_reply": "2025-03-17T09:43:49.391599Z",
     "shell.execute_reply.started": "2025-03-17T09:43:40.105225Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "| table_schema|          table_name|\n",
      "+-------------+--------------------+\n",
      "|classicmodels|           employees|\n",
      "|classicmodels|             offices|\n",
      "|classicmodels|           customers|\n",
      "|classicmodels|        orderdetails|\n",
      "|classicmodels|        productlines|\n",
      "|classicmodels|            products|\n",
      "|classicmodels|              orders|\n",
      "|   pg_catalog|             pg_type|\n",
      "|classicmodels|            payments|\n",
      "|   pg_catalog|    pg_foreign_table|\n",
      "|   pg_catalog|            pg_roles|\n",
      "|   pg_catalog|         pg_settings|\n",
      "|   pg_catalog|pg_backend_memory...|\n",
      "|   pg_catalog|pg_shmem_allocations|\n",
      "|   pg_catalog|           pg_tables|\n",
      "|   pg_catalog|pg_statio_all_seq...|\n",
      "|   pg_catalog|     pg_subscription|\n",
      "|   pg_catalog|        pg_attribute|\n",
      "|   pg_catalog|             pg_proc|\n",
      "|   pg_catalog|            pg_class|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%pyspark\n",
    "information_tables_df = spark.read.jdbc(jdbc_url, \"information_schema.tables\", properties=jdbc_properties)\n",
    "information_tables_df.select([\"table_schema\", \"table_name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Now that you listed the available schemas, call the `information_schema.schemata` table. Select the `schema_name` and `schema_owner`, and then show the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:49.394048Z",
     "iopub.status.busy": "2025-03-17T09:43:49.393797Z",
     "iopub.status.idle": "2025-03-17T09:43:50.171928Z",
     "shell.execute_reply": "2025-03-17T09:43:50.171103Z",
     "shell.execute_reply.started": "2025-03-17T09:43:49.394013Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb0c39b094f4f779005ef42795f18cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|         schema_name|     schema_owner|\n",
      "+--------------------+-----------------+\n",
      "|              public|pg_database_owner|\n",
      "|       classicmodels|     postgresuser|\n",
      "|         test_schema|     postgresuser|\n",
      "|classicmodels_sta...|     postgresuser|\n",
      "|  information_schema|         rdsadmin|\n",
      "|          pg_catalog|         rdsadmin|\n",
      "+--------------------+-----------------+"
     ]
    }
   ],
   "source": [
    "information_schemas_df = spark.read.jdbc(jdbc_url, \"information_schema.schemata\", properties=jdbc_properties)\n",
    "information_schemas_df.select([\"schema_name\", \"schema_owner\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Apache Spark 101\n",
    "\n",
    "As mentioned before, Apache Spark is a workload engine that can be used with high-level APIs in programming languages such as Java, Scala and Python. The core abstraction of Spark is a **Resilient Distributed Dataset (RDD)**, which is a collection of elements that can be partitioned across the nodes of the cluster; this enables running intensive workloads on the data in parallel. Spark also has some high-level toolsets like Spark SQL for structured data processing using SQL, pandas API for Spark to run pandas workloads, and MLlib for machine learning workloads. \n",
    "\n",
    "You will focus on PySpark, the Python API. However, you will skip the details on how to connect to a Spark cluster and most of the initial setup required as the necessary configuration to run this notebook has already been provided. \n",
    "\n",
    "In this section, you will get a brief overview of the required classes to access Spark with PySpark and run your workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Spark Classes\n",
    "\n",
    "To start a Spark program you must create a `SparkConf` object, that contains information about your application, and a `SparkContext` object, which tells Spark how to access a cluster.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "The `appName` parameter is a string with the name of your application, it shows on the Spark cluster UI. `master` is the connection string to a Spark cluster or a `\"local\"` string to run in local mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Spark DataFrame\n",
    "\n",
    "In this lab, you will be using the PySpark DataFrame API, which enables the use of Spark DataFrames, an abstraction on top of RDDs. For PySpark applications running this API, you can start by initializing a `SparkSession` object which is the entry point of PySpark.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "In this notebook, you can access a preconfigured `SparkSession` object using the `spark` variable, which you have used before to read the available tables. Now, you will start looking into the Spark DataFrame. Let's read the `orders` table from the `classicmodels` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:50.174185Z",
     "iopub.status.busy": "2025-03-17T09:43:50.173741Z",
     "iopub.status.idle": "2025-03-17T09:43:50.422087Z",
     "shell.execute_reply": "2025-03-17T09:43:50.421327Z",
     "shell.execute_reply.started": "2025-03-17T09:43:50.174146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pyspark\n",
    "\n",
    "# Read data from RDS into a Spark DataFrame\n",
    "orders_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.orders\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the DataFrame operations available, here is a list of some of the most important ones:\n",
    "\n",
    "* `df.printSchema()`: Prints the schema of the DataFrame.\n",
    "* `df.select(\"col\")`: Select the column of the name `col` from the DataFrame.\n",
    "* `df.show()`: Prints the content of the DataFrame.\n",
    "* `df.filter(df[\"col\"] > value)`: Filters the DataFrame based on a logical condition.\n",
    "* `df.groupBy(\"col\").agg()`: Perform an aggregation based on a column of name `col`. The aggregation can be `count`, `max`, `min`, `avg`.\n",
    "* `df.withColumn(\"new_col\",col_values)`: Adds a new column to the DataFrame with the `new_col` name and `col_values` as values for the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by printing the content of the `orders` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:50.429261Z",
     "iopub.status.busy": "2025-03-17T09:43:50.426661Z",
     "iopub.status.idle": "2025-03-17T09:43:51.744963Z",
     "shell.execute_reply": "2025-03-17T09:43:51.744333Z",
     "shell.execute_reply.started": "2025-03-17T09:43:50.429214Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd300c8bd974494be3c6555a7ebc01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+\n",
      "|ordernumber|          orderdate|       requireddate|        shippeddate| status|            comments|customernumber|\n",
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06 00:00:00|2003-01-13 00:00:00|2003-01-10 00:00:00|Shipped|                NULL|           363|\n",
      "|      10101|2003-01-09 00:00:00|2003-01-18 00:00:00|2003-01-11 00:00:00|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10 00:00:00|2003-01-18 00:00:00|2003-01-14 00:00:00|Shipped|                NULL|           181|\n",
      "|      10103|2003-01-29 00:00:00|2003-02-07 00:00:00|2003-02-02 00:00:00|Shipped|                NULL|           121|\n",
      "|      10104|2003-01-31 00:00:00|2003-02-09 00:00:00|2003-02-01 00:00:00|Shipped|                NULL|           141|\n",
      "|      10105|2003-02-11 00:00:00|2003-02-21 00:00:00|2003-02-12 00:00:00|Shipped|                NULL|           145|\n",
      "|      10106|2003-02-17 00:00:00|2003-02-24 00:00:00|2003-02-21 00:00:00|Shipped|                NULL|           278|\n",
      "|      10107|2003-02-24 00:00:00|2003-03-03 00:00:00|2003-02-26 00:00:00|Shipped|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03 00:00:00|2003-03-12 00:00:00|2003-03-08 00:00:00|Shipped|                NULL|           385|\n",
      "|      10109|2003-03-10 00:00:00|2003-03-19 00:00:00|2003-03-11 00:00:00|Shipped|Customer requeste...|           486|\n",
      "|      10110|2003-03-18 00:00:00|2003-03-24 00:00:00|2003-03-20 00:00:00|Shipped|                NULL|           187|\n",
      "|      10111|2003-03-25 00:00:00|2003-03-31 00:00:00|2003-03-30 00:00:00|Shipped|                NULL|           129|\n",
      "|      10112|2003-03-24 00:00:00|2003-04-03 00:00:00|2003-03-29 00:00:00|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26 00:00:00|2003-04-02 00:00:00|2003-03-27 00:00:00|Shipped|                NULL|           124|\n",
      "|      10114|2003-04-01 00:00:00|2003-04-07 00:00:00|2003-04-02 00:00:00|Shipped|                NULL|           172|\n",
      "|      10115|2003-04-04 00:00:00|2003-04-12 00:00:00|2003-04-07 00:00:00|Shipped|                NULL|           424|\n",
      "|      10116|2003-04-11 00:00:00|2003-04-19 00:00:00|2003-04-13 00:00:00|Shipped|                NULL|           381|\n",
      "|      10117|2003-04-16 00:00:00|2003-04-24 00:00:00|2003-04-17 00:00:00|Shipped|                NULL|           148|\n",
      "|      10118|2003-04-21 00:00:00|2003-04-29 00:00:00|2003-04-26 00:00:00|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28 00:00:00|2003-05-05 00:00:00|2003-05-02 00:00:00|Shipped|                NULL|           382|\n",
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a Spark DataFrame from collections such as a list of tuples, a list of dictionaries, an RDD, and a `pandas` DataFrame. This is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:51.746112Z",
     "iopub.status.busy": "2025-03-17T09:43:51.745948Z",
     "iopub.status.idle": "2025-03-17T09:43:54.056684Z",
     "shell.execute_reply": "2025-03-17T09:43:54.056032Z",
     "shell.execute_reply.started": "2025-03-17T09:43:51.746091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e77d386b99f40f4b820887dd574dc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Carla|  3|\n",
      "+-----+---+"
     ]
    }
   ],
   "source": [
    "list_of_tuples = [(\"Alice\", 1),(\"Bob\", 2),(\"Carla\", 3)]\n",
    "spark.createDataFrame(list_of_tuples).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the schema as a second parameter, either by passing a list of column names or a `StructType`, the later one uses an array of `StructField` for each column with the corresponding name, type and if the column accepts nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:54.058083Z",
     "iopub.status.busy": "2025-03-17T09:43:54.057898Z",
     "iopub.status.idle": "2025-03-17T09:43:54.820837Z",
     "shell.execute_reply": "2025-03-17T09:43:54.820150Z",
     "shell.execute_reply.started": "2025-03-17T09:43:54.058060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecaa74fafd57406d88df309b7801f974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Carla|  3|\n",
      "+-----+---+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "   StructField(\"name\", StringType(), True),\n",
    "   StructField(\"age\", IntegerType(), True)])\n",
    "test_df = spark.createDataFrame(list_of_tuples, schema)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also write Spark DataFrames in the same formats that you can read. During this lab, you will save DataFrames to the same database. Here is an example of how to store a DataFrame to Postgres to a pre-created `test_schema`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:54.822201Z",
     "iopub.status.busy": "2025-03-17T09:43:54.822007Z",
     "iopub.status.idle": "2025-03-17T09:43:57.119221Z",
     "shell.execute_reply": "2025-03-17T09:43:57.118602Z",
     "shell.execute_reply.started": "2025-03-17T09:43:54.822176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc2ac8d483a414e8cc4bbb0865e2bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df.write.jdbc(url=jdbc_url, table=\"test_schema.test_table\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Spark SQL\n",
    "\n",
    "As mentioned before, one of the high-level tools offered by Spark is Spark SQL, this will be one of the main tools you will use during the lab. You can perform SQL queries using the available DataFrames, first, you have to register each DataFrame as a temporary view and then call the `sql` function from the `SparkSession` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:43:57.121003Z",
     "iopub.status.busy": "2025-03-17T09:43:57.120630Z",
     "iopub.status.idle": "2025-03-17T09:43:57.923081Z",
     "shell.execute_reply": "2025-03-17T09:43:57.922471Z",
     "shell.execute_reply.started": "2025-03-17T09:43:57.120966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e77e8d2fa5d41e39a24c6c796075440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+\n",
      "|ordernumber|          orderdate|       requireddate|        shippeddate| status|            comments|customernumber|\n",
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06 00:00:00|2003-01-13 00:00:00|2003-01-10 00:00:00|Shipped|                NULL|           363|\n",
      "|      10101|2003-01-09 00:00:00|2003-01-18 00:00:00|2003-01-11 00:00:00|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10 00:00:00|2003-01-18 00:00:00|2003-01-14 00:00:00|Shipped|                NULL|           181|\n",
      "|      10103|2003-01-29 00:00:00|2003-02-07 00:00:00|2003-02-02 00:00:00|Shipped|                NULL|           121|\n",
      "|      10104|2003-01-31 00:00:00|2003-02-09 00:00:00|2003-02-01 00:00:00|Shipped|                NULL|           141|\n",
      "|      10105|2003-02-11 00:00:00|2003-02-21 00:00:00|2003-02-12 00:00:00|Shipped|                NULL|           145|\n",
      "|      10106|2003-02-17 00:00:00|2003-02-24 00:00:00|2003-02-21 00:00:00|Shipped|                NULL|           278|\n",
      "|      10107|2003-02-24 00:00:00|2003-03-03 00:00:00|2003-02-26 00:00:00|Shipped|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03 00:00:00|2003-03-12 00:00:00|2003-03-08 00:00:00|Shipped|                NULL|           385|\n",
      "|      10109|2003-03-10 00:00:00|2003-03-19 00:00:00|2003-03-11 00:00:00|Shipped|Customer requeste...|           486|\n",
      "|      10110|2003-03-18 00:00:00|2003-03-24 00:00:00|2003-03-20 00:00:00|Shipped|                NULL|           187|\n",
      "|      10111|2003-03-25 00:00:00|2003-03-31 00:00:00|2003-03-30 00:00:00|Shipped|                NULL|           129|\n",
      "|      10112|2003-03-24 00:00:00|2003-04-03 00:00:00|2003-03-29 00:00:00|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26 00:00:00|2003-04-02 00:00:00|2003-03-27 00:00:00|Shipped|                NULL|           124|\n",
      "|      10114|2003-04-01 00:00:00|2003-04-07 00:00:00|2003-04-02 00:00:00|Shipped|                NULL|           172|\n",
      "|      10115|2003-04-04 00:00:00|2003-04-12 00:00:00|2003-04-07 00:00:00|Shipped|                NULL|           424|\n",
      "|      10116|2003-04-11 00:00:00|2003-04-19 00:00:00|2003-04-13 00:00:00|Shipped|                NULL|           381|\n",
      "|      10117|2003-04-16 00:00:00|2003-04-24 00:00:00|2003-04-17 00:00:00|Shipped|                NULL|           148|\n",
      "|      10118|2003-04-21 00:00:00|2003-04-29 00:00:00|2003-04-26 00:00:00|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28 00:00:00|2003-05-05 00:00:00|2003-05-02 00:00:00|Shipped|                NULL|           382|\n",
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM orders\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - UDFs and Data Types\n",
    "\n",
    "Another advantage of Spark SQL is the definition of Python functions as SQL User Defined Functions (UDF). UDFs help us store custom logic and use it with multiple DataFrames. For example, if you want a text column to be title case (the first letter of each word is capitalized), you can define a function for it and then store it as a UDF.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def titleCase(text: str):\n",
    "    output = ' '.join(word[0].upper() + word[1:] for word in text.split())\n",
    "    return output\n",
    "\n",
    "spark.udf.register(\"titleUDF\", titleCase, StringType())\n",
    "\n",
    "spark.sql(\"select book_id, titleUDF(book_name) as title from books\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, you registered the UDF using the `spark.udf.register` function, which takes the name of the function to use in SQL, the Python function and the return type. You use Spark SQL Data Types in this case, you will work more with them later, as they can be used to describe the schema of a Spark DataFrame. It's also worth mentioning that you can also use UDF directly on DataFrames; in this case, we use a `lambda` function.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "titleUDF = udf(lambda z: titleCase(z),StringType())\n",
    "\n",
    "books_df.select(col(\"book_id\"), titleUDF(col(\"book_name\")).alias(\"title\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a UDF using this method to generate surrogate keys. You will use the `hashlib` library to generate a hash based on a list of column values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:45:04.734216Z",
     "iopub.status.busy": "2025-03-17T09:45:04.733995Z",
     "iopub.status.idle": "2025-03-17T09:45:04.791316Z",
     "shell.execute_reply": "2025-03-17T09:45:04.790529Z",
     "shell.execute_reply.started": "2025-03-17T09:45:04.734191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b85c1fe224418bbb4de744e6720030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import List\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, udf, array\n",
    "\n",
    "def surrogateKey(text_values: List[str]):\n",
    "    sha256 = hashlib.sha256()\n",
    "    data = ''.join(text_values)\n",
    "    sha256.update(data.encode())\n",
    "    return sha256.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function, create an array of strings, and call the function with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:45:05.307992Z",
     "iopub.status.busy": "2025-03-17T09:45:05.307775Z",
     "iopub.status.idle": "2025-03-17T09:45:05.365445Z",
     "shell.execute_reply": "2025-03-17T09:45:05.364564Z",
     "shell.execute_reply.started": "2025-03-17T09:45:05.307967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b4e30927674746901b252ceeaea49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ba6de314675567f28b142ba42bab0ab026f777507ab8ca885397dd9494d2a855'"
     ]
    }
   ],
   "source": [
    "surrogateKey([\"01221212\",\"123123123\",\"Hello World\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the UDF with the `surrogateKey` function and a lambda function, the return type is `StringType()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:45:06.465482Z",
     "iopub.status.busy": "2025-03-17T09:45:06.465266Z",
     "iopub.status.idle": "2025-03-17T09:45:06.516095Z",
     "shell.execute_reply": "2025-03-17T09:45:06.515565Z",
     "shell.execute_reply.started": "2025-03-17T09:45:06.465459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b33f762114d4dc280f244c8c52737a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "surrogateUDF = udf(lambda z: surrogateKey(z),StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this new UDF with the `orders_df`, use the `withColumn` function to generate a new column `order_key`, and pass to the UDF the `ordernumber` and `status` column as an `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:45:06.908721Z",
     "iopub.status.busy": "2025-03-17T09:45:06.908494Z",
     "iopub.status.idle": "2025-03-17T09:45:16.228307Z",
     "shell.execute_reply": "2025-03-17T09:45:16.227578Z",
     "shell.execute_reply.started": "2025-03-17T09:45:06.908696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff634c2bf23403ebde191529f96ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+--------------------+\n",
      "|ordernumber|          orderdate|       requireddate|        shippeddate| status|            comments|customernumber|           order_key|\n",
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+--------------------+\n",
      "|      10100|2003-01-06 00:00:00|2003-01-13 00:00:00|2003-01-10 00:00:00|Shipped|                NULL|           363|80da0f716659e6a4e...|\n",
      "|      10101|2003-01-09 00:00:00|2003-01-18 00:00:00|2003-01-11 00:00:00|Shipped|Check on availabi...|           128|3d9d10b94ae5f34b7...|\n",
      "|      10102|2003-01-10 00:00:00|2003-01-18 00:00:00|2003-01-14 00:00:00|Shipped|                NULL|           181|8fd5dcf6d74c5bec5...|\n",
      "|      10103|2003-01-29 00:00:00|2003-02-07 00:00:00|2003-02-02 00:00:00|Shipped|                NULL|           121|c1e90abbd505a6b80...|\n",
      "|      10104|2003-01-31 00:00:00|2003-02-09 00:00:00|2003-02-01 00:00:00|Shipped|                NULL|           141|5ee04b501bd77201b...|\n",
      "|      10105|2003-02-11 00:00:00|2003-02-21 00:00:00|2003-02-12 00:00:00|Shipped|                NULL|           145|000fa890acda167c2...|\n",
      "|      10106|2003-02-17 00:00:00|2003-02-24 00:00:00|2003-02-21 00:00:00|Shipped|                NULL|           278|f46687ca5dead186a...|\n",
      "|      10107|2003-02-24 00:00:00|2003-03-03 00:00:00|2003-02-26 00:00:00|Shipped|Difficult to nego...|           131|5fe6940228fe99cc3...|\n",
      "|      10108|2003-03-03 00:00:00|2003-03-12 00:00:00|2003-03-08 00:00:00|Shipped|                NULL|           385|3d407b10c7ba922cf...|\n",
      "|      10109|2003-03-10 00:00:00|2003-03-19 00:00:00|2003-03-11 00:00:00|Shipped|Customer requeste...|           486|fa5e626bc0b2aed77...|\n",
      "|      10110|2003-03-18 00:00:00|2003-03-24 00:00:00|2003-03-20 00:00:00|Shipped|                NULL|           187|ad200e71b73dc488b...|\n",
      "|      10111|2003-03-25 00:00:00|2003-03-31 00:00:00|2003-03-30 00:00:00|Shipped|                NULL|           129|3063e95f202c5e180...|\n",
      "|      10112|2003-03-24 00:00:00|2003-04-03 00:00:00|2003-03-29 00:00:00|Shipped|Customer requeste...|           144|ab9e83d5b06dd548b...|\n",
      "|      10113|2003-03-26 00:00:00|2003-04-02 00:00:00|2003-03-27 00:00:00|Shipped|                NULL|           124|32b83351e60c7c83f...|\n",
      "|      10114|2003-04-01 00:00:00|2003-04-07 00:00:00|2003-04-02 00:00:00|Shipped|                NULL|           172|3c5415314bb1353f4...|\n",
      "|      10115|2003-04-04 00:00:00|2003-04-12 00:00:00|2003-04-07 00:00:00|Shipped|                NULL|           424|0db2a99d723315f16...|\n",
      "|      10116|2003-04-11 00:00:00|2003-04-19 00:00:00|2003-04-13 00:00:00|Shipped|                NULL|           381|8b3bdf85986bac860...|\n",
      "|      10117|2003-04-16 00:00:00|2003-04-24 00:00:00|2003-04-17 00:00:00|Shipped|                NULL|           148|d7a854d5bd57e814b...|\n",
      "|      10118|2003-04-21 00:00:00|2003-04-29 00:00:00|2003-04-26 00:00:00|Shipped|Customer has work...|           216|168ed65bc7924b81b...|\n",
      "|      10119|2003-04-28 00:00:00|2003-05-05 00:00:00|2003-05-02 00:00:00|Shipped|                NULL|           382|c5bd680c735e6e25c...|\n",
      "+-----------+-------------------+-------------------+-------------------+-------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "orders_df.withColumn(\"order_key\", surrogateUDF(array(orders_df.ordernumber,orders_df.status))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Data Modeling with Spark\n",
    "\n",
    "As mentioned before, you will recreate the star schema from the Week 1 assignment of this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 - Read the Tables\n",
    "\n",
    "For the first step, you will read the `classicmodels` tables with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:48:03.087269Z",
     "iopub.status.busy": "2025-03-17T09:48:03.087047Z",
     "iopub.status.idle": "2025-03-17T09:48:03.844994Z",
     "shell.execute_reply": "2025-03-17T09:48:03.844348Z",
     "shell.execute_reply.started": "2025-03-17T09:48:03.087244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59d6962b9914157ac889a2e972165e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "employees_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.employees\", properties=jdbc_properties)\n",
    "offices_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.offices\", properties=jdbc_properties)\n",
    "customers_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.customers\", properties=jdbc_properties)\n",
    "orderdetails_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.orderdetails\", properties=jdbc_properties)\n",
    "productlines_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.productlines\", properties=jdbc_properties)\n",
    "products_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.products\", properties=jdbc_properties)\n",
    "payments_df = spark.read.jdbc(url=jdbc_url, table=\"classicmodels.payments\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the Spark DataFrames as temporary views, and call them with the same name as their Postgres RDS counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T09:48:07.256132Z",
     "iopub.status.busy": "2025-03-17T09:48:07.255903Z",
     "iopub.status.idle": "2025-03-17T09:48:07.517383Z",
     "shell.execute_reply": "2025-03-17T09:48:07.516830Z",
     "shell.execute_reply.started": "2025-03-17T09:48:07.256108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebe685ea52b454ba6ad05e491910155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "offices_df.createOrReplaceTempView(\"offices\")\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orderdetails_df.createOrReplaceTempView(\"orderdetails\")\n",
    "productlines_df.createOrReplaceTempView(\"productlines\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "payments_df.createOrReplaceTempView(\"payments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the schema of any of the tables with the `printSchema` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:10:38.960978Z",
     "iopub.status.busy": "2025-03-17T10:10:38.960750Z",
     "iopub.status.idle": "2025-03-17T10:10:39.006092Z",
     "shell.execute_reply": "2025-03-17T10:10:39.005482Z",
     "shell.execute_reply.started": "2025-03-17T10:10:38.960955Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7519fe5427524e8da283ffce56c82050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productcode: string (nullable = true)\n",
      " |-- productname: string (nullable = true)\n",
      " |-- productscale: string (nullable = true)\n",
      " |-- productvendor: string (nullable = true)\n",
      " |-- productdescription: string (nullable = true)\n",
      " |-- quantityinstock: short (nullable = true)\n",
      " |-- buyprice: decimal(38,18) (nullable = true)\n",
      " |-- msrp: decimal(38,18) (nullable = true)\n",
      " |-- productline: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 - Star Schema\n",
    "\n",
    "Here is the new ERM diagram for the star schema, which was modified to include some transformations. You will use Spark SQL to bring the necessary columns for each table. Then you will perform additional operations to the resulting DataFrame and store the resulting data in the `classicmodels_star_schema` schema in the Postgres RDS.\n",
    "\n",
    "![img](https://dlai-data-engineering.s3.amazonaws.com/labs/c4w3a1-177787/images/star_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.3'></a>\n",
    "### 4.3 - Customers Dimension\n",
    "\n",
    "Let's start with the dimensional tables, first with the `customers` dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.1. Create a SQL query that brings the relevant columns from the `customers` temporal view and stores the query result in a Spark DataFrame. Follow the instructions to prepare the query:\n",
    "- You need to create a column `customer_number` based on the `customerNumber`, which will be a surrogate key `customer_key` later. This function requires an array of text columns so you will need to `cast()` the `customerNumber` to string.\n",
    "- For this new data model you are required to create the field `contact_name` which is a combination of the `contactFirstName` and `contactLastName` fields. You can use the function `concat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:10:53.192871Z",
     "iopub.status.busy": "2025-03-17T10:10:53.192638Z",
     "iopub.status.idle": "2025-03-17T10:10:53.448507Z",
     "shell.execute_reply": "2025-03-17T10:10:53.447835Z",
     "shell.execute_reply.started": "2025-03-17T10:10:53.192845Z"
    },
    "exercise": [
     "ex01"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d2a8ab12774569acbc6c089a3c280e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_query_customers = \"\"\"\n",
    "SELECT \n",
    "    cast(customerNumber as string) as customer_number, \n",
    "    customerName as customer_name,\n",
    "    concat(contactFirstName, contactLastName) as contact_name, \n",
    "    phone as phone, \n",
    "    addressLine1 as address_line_1, \n",
    "    addressLine2 as address_line_2, \n",
    "    postalCode as postal_code, \n",
    "    city as city, \n",
    "    state as state, \n",
    "    country as country,\n",
    "    creditLimit as credit_limit\n",
    "FROM customers\n",
    "\"\"\"\n",
    "\n",
    "dim_customers_df = spark.sql(select_query_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2. Now, with the resulting `dim_customers_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `customer_number`. You will need to use `array()` function to convert it to an array. \n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `customer_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:10:54.730709Z",
     "iopub.status.busy": "2025-03-17T10:10:54.730490Z",
     "iopub.status.idle": "2025-03-17T10:10:54.988850Z",
     "shell.execute_reply": "2025-03-17T10:10:54.988236Z",
     "shell.execute_reply.started": "2025-03-17T10:10:54.730685Z"
    },
    "exercise": [
     "ex02"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4083c2c46a043389ea317f849e40f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_customers_df = dim_customers_df.withColumn(\"customer_key\", surrogateUDF(array(dim_customers_df.customer_number)))\\\n",
    ".select([\"customer_key\",\"customer_name\",\"contact_name\",\"phone\",\"address_line_1\",\"address_line_2\",\"postal_code\",\"city\",\"state\",\"country\",\"credit_limit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.3. Now, store the final DataFrame into the `classicmodels_star_schema` schema, creating a new table called `dim_customers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:10:56.882338Z",
     "iopub.status.busy": "2025-03-17T10:10:56.882055Z",
     "iopub.status.idle": "2025-03-17T10:11:00.159533Z",
     "shell.execute_reply": "2025-03-17T10:11:00.158916Z",
     "shell.execute_reply.started": "2025-03-17T10:10:56.882307Z"
    },
    "exercise": [
     "ex03"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa183443fe35422088b5bc897f3d8eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_customers_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_customers\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.4. Check that the table is stored in the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:11:00.161208Z",
     "iopub.status.busy": "2025-03-17T10:11:00.160966Z",
     "iopub.status.idle": "2025-03-17T10:11:01.424167Z",
     "shell.execute_reply": "2025-03-17T10:11:01.423389Z",
     "shell.execute_reply.started": "2025-03-17T10:11:00.161172Z"
    },
    "exercise": [
     "ex04"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee6b6a1ee64945bc74d2082dd70199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_customers column names:  ['customer_key', 'customer_name', 'contact_name', 'phone', 'address_line_1', 'address_line_2', 'postal_code', 'city', 'state', 'country', 'credit_limit']\n",
      "dim_customers number of rows:  122"
     ]
    }
   ],
   "source": [
    "dim_customers_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_customers\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_customers column names: \", dim_customers_df_check.columns)\n",
    "\n",
    "dim_customers_row_count = dim_customers_df_check.count()\n",
    "print(\"dim_customers number of rows: \", dim_customers_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_customers column names:  ['customer_key', 'customer_name', 'contact_name', 'phone', 'address_line_1', 'address_line_2', 'postal_code', 'city', 'state', 'country', 'credit_limit']\n",
    "dim_customers number of rows:  122\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.4'></a>\n",
    "### 4.4 - Products Dimension\n",
    "\n",
    "Continue with the `products` dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.1. Create a SQL query that brings the relevant columns from the `products` temporal view and stores the query result in a Spark DataFrame. Later you will create a surrogate key `product_key` based on the `productCode`. The `productCode` is already a string, so you don't have to cast it - just select it for now and name as `product_code` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:11:54.975862Z",
     "iopub.status.busy": "2025-03-17T10:11:54.975620Z",
     "iopub.status.idle": "2025-03-17T10:11:55.228125Z",
     "shell.execute_reply": "2025-03-17T10:11:55.227483Z",
     "shell.execute_reply.started": "2025-03-17T10:11:54.975837Z"
    },
    "exercise": [
     "ex05"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1d295ac14043e884885cd4fd8c7143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_query_products = \"\"\"\n",
    "SELECT \n",
    "    productCode as product_code, \n",
    "    productName as product_name, \n",
    "    products.productLine as product_line, \n",
    "    productScale as product_scale, \n",
    "    productVendor as product_vendor,\n",
    "    productDescription as product_description, \n",
    "    textDescription as product_line_description\n",
    "FROM products\n",
    "JOIN productlines ON products.productLine=productlines.productLine\n",
    "\"\"\"\n",
    "\n",
    "dim_products_df = spark.sql(select_query_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.2. With the resulting `dim_products_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `product_code`. You will need to use `array()` function to convert it to an array. \n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `product_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:12:20.091246Z",
     "iopub.status.busy": "2025-03-17T10:12:20.090964Z",
     "iopub.status.idle": "2025-03-17T10:12:20.340166Z",
     "shell.execute_reply": "2025-03-17T10:12:20.339433Z",
     "shell.execute_reply.started": "2025-03-17T10:12:20.091215Z"
    },
    "exercise": [
     "ex06"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be4a1d0696f433ba50c0fbffbf5ce35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_products_df = dim_products_df.withColumn(\"product_key\", surrogateUDF(array(dim_products_df.product_code)))\\\n",
    ".select([\"product_key\",\"product_name\",\"product_line\",\"product_scale\",\"product_vendor\",\"product_description\",\"product_line_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.3. Store the `dim_products_df` DataFrame into the `classicmodels_star_schema` schema, table `dim_products` (see how it was done in the step 4.3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:12:48.429350Z",
     "iopub.status.busy": "2025-03-17T10:12:48.429111Z",
     "iopub.status.idle": "2025-03-17T10:13:05.747693Z",
     "shell.execute_reply": "2025-03-17T10:13:05.747033Z",
     "shell.execute_reply.started": "2025-03-17T10:12:48.429325Z"
    },
    "exercise": [
     "ex07"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597d13417d5841c9b2596c71349dc8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_products_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_products_df\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.4. Check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:13:33.822281Z",
     "iopub.status.busy": "2025-03-17T10:13:33.822046Z",
     "iopub.status.idle": "2025-03-17T10:13:34.103870Z",
     "shell.execute_reply": "2025-03-17T10:13:34.103101Z",
     "shell.execute_reply.started": "2025-03-17T10:13:33.822256Z"
    },
    "exercise": [
     "ex08"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4030a4ccfd324c138ab6f504cf444613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_products column names:  ['product_key', 'product_name', 'product_line', 'product_scale', 'product_vendor', 'product_description', 'product_line_description']\n",
      "dim_products number of rows:  110"
     ]
    }
   ],
   "source": [
    "dim_products_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_products_df\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_products column names: \", dim_products_df_check.columns)\n",
    "\n",
    "dim_products_row_count = dim_products_df_check.count()\n",
    "print(\"dim_products number of rows: \", dim_products_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_products column names:  ['product_key', 'product_name', 'product_line', 'product_scale', 'product_vendor', 'product_description', 'product_line_description']\n",
    "dim_products number of rows:  110\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.5'></a>\n",
    "### 4.5 - Offices Dimension\n",
    "\n",
    "Now, let's proceed with the `offices` dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.1. Create a SQL query that brings the relevant columns from the `offices` temporal table and stores the query result in a Spark dataframe. Later you will create a surrogate key `office_key` based on the `officeCode`. The `officeCode` is already a string, so you don't have to cast it - just select it for now and name as `office_code` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:13:50.427938Z",
     "iopub.status.busy": "2025-03-17T10:13:50.427711Z",
     "iopub.status.idle": "2025-03-17T10:13:50.478621Z",
     "shell.execute_reply": "2025-03-17T10:13:50.477944Z",
     "shell.execute_reply.started": "2025-03-17T10:13:50.427915Z"
    },
    "exercise": [
     "ex09"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26075c69d58e479ca281bac7e600eea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_query_offices = \"\"\"\n",
    "SELECT \n",
    "    officeCode as office_code, \n",
    "    postalCode as postal_code, \n",
    "    city as city, \n",
    "    state as state, \n",
    "    country as country, \n",
    "    territory as territory\n",
    "FROM offices\n",
    "\"\"\"\n",
    "\n",
    "dim_offices_df = spark.sql(select_query_offices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.2. With the resulting `dim_offices_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `office_code`. You will need to use `array()` function to convert it to an array. \n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `office_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:14:12.673676Z",
     "iopub.status.busy": "2025-03-17T10:14:12.673435Z",
     "iopub.status.idle": "2025-03-17T10:14:12.934999Z",
     "shell.execute_reply": "2025-03-17T10:14:12.934201Z",
     "shell.execute_reply.started": "2025-03-17T10:14:12.673650Z"
    },
    "exercise": [
     "ex10"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64fb648729e447d9db5e27e9f60d902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_offices_df = dim_offices_df.withColumn(\"office_key\", surrogateUDF(array(dim_offices_df.office_code)))\\\n",
    ".select([\"office_key\",\"postal_code\",\"city\",\"state\",\"country\",\"territory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.3. Store the `dim_offices_df` DataFrame into the `classicmodels_star_schema` schema, table `dim_offices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:14:26.363912Z",
     "iopub.status.busy": "2025-03-17T10:14:26.363673Z",
     "iopub.status.idle": "2025-03-17T10:14:27.119325Z",
     "shell.execute_reply": "2025-03-17T10:14:27.118693Z",
     "shell.execute_reply.started": "2025-03-17T10:14:26.363885Z"
    },
    "exercise": [
     "ex11"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d5b4f341744622906989ad3bbbab03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_offices_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_offices_df\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.4. Check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:14:38.896466Z",
     "iopub.status.busy": "2025-03-17T10:14:38.896242Z",
     "iopub.status.idle": "2025-03-17T10:14:39.161365Z",
     "shell.execute_reply": "2025-03-17T10:14:39.160493Z",
     "shell.execute_reply.started": "2025-03-17T10:14:38.896441Z"
    },
    "exercise": [
     "ex12"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a299e9bb67f84c07b644a321c4b15ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_offices column names:  ['office_key', 'postal_code', 'city', 'state', 'country', 'territory']\n",
      "dim_offices number of rows:  7"
     ]
    }
   ],
   "source": [
    "dim_offices_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_offices_df\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_offices column names: \", dim_offices_df_check.columns)\n",
    "\n",
    "dim_offices_row_count = dim_offices_df_check.count()\n",
    "print(\"dim_offices number of rows: \", dim_offices_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_offices column names:  ['office_key', 'postal_code', 'city', 'state', 'country', 'territory']\n",
    "dim_offices number of rows:  7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.6'></a>\n",
    "### 4.6 - Employees Dimension\n",
    "\n",
    "Let's continue with the `employees` dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6.1. Follow similar steps to create `employees` dimension. There will be a surrogate key `employee_key` based on the `employeeNumber`. You'll need to create a column `employee_number` based on the `employeeNumber`. Cast to string with the function `cast()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:15:14.903747Z",
     "iopub.status.busy": "2025-03-17T10:15:14.903410Z",
     "iopub.status.idle": "2025-03-17T10:15:14.941099Z",
     "shell.execute_reply": "2025-03-17T10:15:14.940435Z",
     "shell.execute_reply.started": "2025-03-17T10:15:14.903703Z"
    },
    "exercise": [
     "ex13"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888c67d6eedf433280379cc3b91a21fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_query_employees = \"\"\"\n",
    "SELECT \n",
    "    cast(employeeNumber as string) as employee_number,\n",
    "    lastName as employee_last_name, \n",
    "    firstName as employee_first_name, \n",
    "    jobTitle as job_title, \n",
    "    email as email\n",
    "FROM employees\n",
    "\"\"\"\n",
    "\n",
    "dim_employees_df = spark.sql(select_query_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6.2. With the resulting `dim_employees_df` DataFrame:\n",
    "- Call the `surrogateUDF` to generate a surrogate key based on the `employee_number`. You will need to use `array()` function to convert it to an array.\n",
    "- Add the surrogate key using the `withColumn()` function, call the new column `employee_key`. \n",
    "\n",
    "Then perform a select to grab the columns related to the ERM diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:15:38.917394Z",
     "iopub.status.busy": "2025-03-17T10:15:38.917168Z",
     "iopub.status.idle": "2025-03-17T10:15:39.165117Z",
     "shell.execute_reply": "2025-03-17T10:15:39.164472Z",
     "shell.execute_reply.started": "2025-03-17T10:15:38.917371Z"
    },
    "exercise": [
     "ex14"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c10483246745f7b669b2d1d49cf828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_employees_df = dim_employees_df.withColumn(\"employee_key\", surrogateUDF(array(dim_employees_df.employee_number)))\\\n",
    ".select([\"employee_key\",\"employee_last_name\",\"employee_first_name\",\"email\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T03:29:22.461906Z",
     "iopub.status.busy": "2024-06-21T03:29:22.461570Z"
    }
   },
   "source": [
    "4.6.3. Store the `dim_employees_df` DataFrame into the `classicmodels_star_schema` schema, table `dim_employees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:15:48.204338Z",
     "iopub.status.busy": "2025-03-17T10:15:48.204009Z",
     "iopub.status.idle": "2025-03-17T10:15:57.502261Z",
     "shell.execute_reply": "2025-03-17T10:15:57.501585Z",
     "shell.execute_reply.started": "2025-03-17T10:15:48.204297Z"
    },
    "exercise": [
     "ex15"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6c07e939494f1891b9aec7978e106b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_employees_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_employees_df\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6.4. Check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:16:00.150663Z",
     "iopub.status.busy": "2025-03-17T10:16:00.150436Z",
     "iopub.status.idle": "2025-03-17T10:16:00.906572Z",
     "shell.execute_reply": "2025-03-17T10:16:00.905848Z",
     "shell.execute_reply.started": "2025-03-17T10:16:00.150641Z"
    },
    "exercise": [
     "ex16"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c415472aa2457abdf954681b16e854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_employees column names:  ['employee_key', 'employee_last_name', 'employee_first_name', 'email']\n",
      "dim_employees number of rows:  23"
     ]
    }
   ],
   "source": [
    "dim_employees_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.dim_employees_df\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_employees column names: \", dim_employees_df_check.columns)\n",
    "\n",
    "dim_employees_row_count = dim_employees_df_check.count()\n",
    "print(\"dim_employees number of rows: \", dim_employees_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_employees column names:  ['employee_key', 'employee_last_name', 'employee_first_name', 'email']\n",
    "dim_employees number of rows:  23\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.7'></a>\n",
    "### 4.7 - Date Dimension\n",
    "\n",
    "4.7.1. As in the `dbt` lab, you will limit the date's dimension table to the dates that appear in the `orders` table. You are already provided with the date range required to create your dimension table. In the following cell, you will:\n",
    "\n",
    "- Use the `to_date` function to enclose the `start_date` and `end_date` strings to convert them into actual date types.\n",
    "- Use the `sequence` function from `psypark.sql.functions` to generate a sequence of values from the `start_date` to the `end_date`. Note that the third parameter is the interval, which has been set to `interval 1 day`. The result from the `sequence` function is an array of values.\n",
    "- Finally, enclose the `sequence` function into the `explode` function. This function takes an array and returns one row for each element in the array. Note how the column has been named `date_day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:16:07.966402Z",
     "iopub.status.busy": "2025-03-17T10:16:07.966182Z",
     "iopub.status.idle": "2025-03-17T10:16:08.229621Z",
     "shell.execute_reply": "2025-03-17T10:16:08.229000Z",
     "shell.execute_reply.started": "2025-03-17T10:16:07.966378Z"
    },
    "exercise": [
     "ex17"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fda0d39a9449569d0b7c339ebe41cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, sequence, year, month, dayofweek, dayofmonth, dayofyear, weekofyear, date_format, lit\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Date range\n",
    "start_date = \"2003-01-01\"\n",
    "end_date = \"2005-12-31\"\n",
    "\n",
    "date_range_df = spark.sql(f\"SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7.2. Based on the `date_range_df` DataFrame, you are going to create the following columns by using the `withColumn()`:\n",
    "\n",
    "- Get the day of the week with the `dayofweek()` function; store it at the column `day_of_week`.\n",
    "- Get the day of the month with the `dayofmonth()` function; store it at the column `day_of_month`.\n",
    "- Get the number of the day in the year with the `dayofyear()` function; store it in the column `day_of_year`.\n",
    "- Get the number of the week in the year with the `weekofyear()` function; store it in the column `week_of_year`.\n",
    "- Get the the month with the `month()` function; store it at the column `month_of_year`.\n",
    "- Get the the year with the `year()` function; store it at the column `year_number`.\n",
    "\n",
    "Also, you will be creating a column `month_name`, but the code is already complete for that.\n",
    "\n",
    "In addition, you are going to create the `quarter_of_year` column by creating a new UDF. This time, instead of using the UDF as an SQL function with SparkSQL, you will register it as a Python UDF. \n",
    "\n",
    "- Complete the `get_quarter_of_year()` function by making an integer division between `date.month - 1` and 3 (with the `//` operator). Then, add 1 and return the value.\n",
    "- Call the `udf()` function and pass it as parameters to the function you just created and the `IntegerType()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:19:53.954397Z",
     "iopub.status.busy": "2025-03-17T10:19:53.954169Z",
     "iopub.status.idle": "2025-03-17T10:19:54.008486Z",
     "shell.execute_reply": "2025-03-17T10:19:54.007628Z",
     "shell.execute_reply.started": "2025-03-17T10:19:53.954372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829a6ab664464963b1535f78954a262b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_quarter_of_year(date):\n",
    "    return (date.month - 1) // 3 + 1\n",
    "\n",
    "get_quarter_of_year_udf = udf(get_quarter_of_year, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:20:58.546107Z",
     "iopub.status.busy": "2025-03-17T10:20:58.545881Z",
     "iopub.status.idle": "2025-03-17T10:21:05.826893Z",
     "shell.execute_reply": "2025-03-17T10:21:05.826258Z",
     "shell.execute_reply.started": "2025-03-17T10:20:58.546082Z"
    },
    "exercise": [
     "ex18"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bff2bd2e3e44512b5657da40ef95734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+-----------+------------+-------------+-----------+----------+---------------+\n",
      "|  date_day|day_of_week|day_of_month|day_of_year|week_of_year|month_of_year|year_number|month_name|quarter_of_year|\n",
      "+----------+-----------+------------+-----------+------------+-------------+-----------+----------+---------------+\n",
      "|2003-01-01|          4|           1|          1|           1|            1|       2003|   January|              1|\n",
      "|2003-01-02|          5|           2|          2|           1|            1|       2003|   January|              1|\n",
      "|2003-01-03|          6|           3|          3|           1|            1|       2003|   January|              1|\n",
      "|2003-01-04|          7|           4|          4|           1|            1|       2003|   January|              1|\n",
      "|2003-01-05|          1|           5|          5|           1|            1|       2003|   January|              1|\n",
      "|2003-01-06|          2|           6|          6|           2|            1|       2003|   January|              1|\n",
      "|2003-01-07|          3|           7|          7|           2|            1|       2003|   January|              1|\n",
      "|2003-01-08|          4|           8|          8|           2|            1|       2003|   January|              1|\n",
      "|2003-01-09|          5|           9|          9|           2|            1|       2003|   January|              1|\n",
      "|2003-01-10|          6|          10|         10|           2|            1|       2003|   January|              1|\n",
      "|2003-01-11|          7|          11|         11|           2|            1|       2003|   January|              1|\n",
      "|2003-01-12|          1|          12|         12|           2|            1|       2003|   January|              1|\n",
      "|2003-01-13|          2|          13|         13|           3|            1|       2003|   January|              1|\n",
      "|2003-01-14|          3|          14|         14|           3|            1|       2003|   January|              1|\n",
      "|2003-01-15|          4|          15|         15|           3|            1|       2003|   January|              1|\n",
      "|2003-01-16|          5|          16|         16|           3|            1|       2003|   January|              1|\n",
      "|2003-01-17|          6|          17|         17|           3|            1|       2003|   January|              1|\n",
      "|2003-01-18|          7|          18|         18|           3|            1|       2003|   January|              1|\n",
      "|2003-01-19|          1|          19|         19|           3|            1|       2003|   January|              1|\n",
      "|2003-01-20|          2|          20|         20|           4|            1|       2003|   January|              1|\n",
      "+----------+-----------+------------+-----------+------------+-------------+-----------+----------+---------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "date_dim_df = date_range_df.withColumn(\"day_of_week\", dayofweek(\"date_day\")) \\\n",
    "    .withColumn(\"day_of_month\", dayofmonth(\"date_day\")) \\\n",
    "    .withColumn(\"day_of_year\", dayofyear(\"date_day\")) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(\"date_day\")) \\\n",
    "    .withColumn(\"month_of_year\", month(\"date_day\")) \\\n",
    "    .withColumn(\"year_number\", year(\"date_day\")) \\\n",
    "    .withColumn(\"month_name\", date_format(\"date_day\", \"MMMM\")) \\\n",
    "    .withColumn(\"quarter_of_year\", get_quarter_of_year_udf(\"date_day\"))\n",
    "\n",
    "# Show the result\n",
    "date_dim_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7.3. Store the `date_dim_df` dataframe into the `classicmodels_star_schema` schema, table `dim_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:22:03.026357Z",
     "iopub.status.busy": "2025-03-17T10:22:03.026029Z",
     "iopub.status.idle": "2025-03-17T10:22:05.302004Z",
     "shell.execute_reply": "2025-03-17T10:22:05.301240Z",
     "shell.execute_reply.started": "2025-03-17T10:22:03.026319Z"
    },
    "exercise": [
     "ex19"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03574f36a5f4a3f9d2793196a22da82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "date_dim_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.date_dim_df\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7.4. Check that your table was correctly stored in the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:22:17.085388Z",
     "iopub.status.busy": "2025-03-17T10:22:17.085161Z",
     "iopub.status.idle": "2025-03-17T10:22:17.858153Z",
     "shell.execute_reply": "2025-03-17T10:22:17.857342Z",
     "shell.execute_reply.started": "2025-03-17T10:22:17.085364Z"
    },
    "exercise": [
     "ex20"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd3843cdf584bb1aabbc4753b798006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_date column names:  ['date_day', 'day_of_week', 'day_of_month', 'day_of_year', 'week_of_year', 'month_of_year', 'year_number', 'month_name', 'quarter_of_year']\n",
      "dim_date number of rows:  1096"
     ]
    }
   ],
   "source": [
    "date_dim_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.date_dim_df\", properties=jdbc_properties)\n",
    "\n",
    "print(\"dim_date column names: \", date_dim_df_check.columns)\n",
    "\n",
    "dim_employees_row_count = date_dim_df_check.count()\n",
    "print(\"dim_date number of rows: \", dim_employees_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "dim_date column names:  ['date_day', 'day_of_week', 'day_of_month', 'day_of_year', 'week_of_year', 'month_of_year', 'year_number', 'month_name', 'quarter_of_year']\n",
    "dim_date number of rows:  1096\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.8'></a>\n",
    "### 4.8 - Fact Table\n",
    "\n",
    "Finally, let's create the orders fact table. Remember that the fact table stores the surrogate keys to the dimensional tables and the numerical facts related to the business process. \n",
    "\n",
    "There has been a change in the model compared to the Week 1 assignment. You will add two new facts:\n",
    "\n",
    "- `profit`: metric calculated by subtracting the price of the product as we sell by the price we bought the product at. \n",
    "- `discount_percentage`: metric calculated by subtracting the MSRP of a product from the selling price, dividing the result by the same MSRP and then multiplying the result by 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.1. Here is the statement to bring all the relevant columns and create the data model into the `fact_table_df` dataframe. There are also corresponding operations to add the missing calculated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:22:23.862065Z",
     "iopub.status.busy": "2025-03-17T10:22:23.861834Z",
     "iopub.status.idle": "2025-03-17T10:22:24.110098Z",
     "shell.execute_reply": "2025-03-17T10:22:24.109461Z",
     "shell.execute_reply.started": "2025-03-17T10:22:23.862041Z"
    },
    "exercise": [
     "ex21"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308ad4fda6414ba79334047cb6db8559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_query_fact = \"\"\"\n",
    "SELECT \n",
    "    orders.orderNumber, \n",
    "    cast(orderdetails.orderLineNumber as string) as order_line_number,\n",
    "    cast(orders.customerNumber as string) as customer_number, \n",
    "    cast(employees.employeeNumber as string) as employee_number,\n",
    "    offices.officeCode,\n",
    "    orderdetails.productCode, \n",
    "    orders.orderDate as order_date,\n",
    "    orders.requiredDate as order_required_date, \n",
    "    orders.shippedDate as order_shipped_date,\n",
    "    orderdetails.quantityOrdered as quantity_ordered, \n",
    "    orderdetails.priceEach as product_price,\n",
    "    (orderdetails.priceEach - products.buyPrice) as profit,\n",
    "    (products.msrp - orderdetails.priceEach)/products.msrp * 100 as discount_percentage\n",
    "FROM orders\n",
    "JOIN orderdetails ON orders.orderNumber = orderdetails.orderNumber\n",
    "JOIN customers ON orders.customerNumber = customers.customerNumber\n",
    "JOIN employees ON customers.salesRepEmployeeNumber = employees.employeeNumber\n",
    "JOIN offices ON employees.officeCode = offices.officeCode\n",
    "JOIN products ON products.productCode = orderdetails.productCode\n",
    "\"\"\";\n",
    "\n",
    "fact_table_df = spark.sql(select_query_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.2. Add the calculated facts and the required surrogate keys. Use function `surrogateUDF()` passing an array based on:\n",
    "- `customer_number` for the `customer_key`,\n",
    "- `employee_number` for the `employee_key`,\n",
    "- `officeCode` for the `office_key`,\n",
    "- `productCode` for the `product_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:23:11.620404Z",
     "iopub.status.busy": "2025-03-17T10:23:11.620170Z",
     "iopub.status.idle": "2025-03-17T10:23:11.873874Z",
     "shell.execute_reply": "2025-03-17T10:23:11.873267Z",
     "shell.execute_reply.started": "2025-03-17T10:23:11.620380Z"
    },
    "exercise": [
     "ex22"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa0d77123b64cd7b7aa34c718ee478b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fact_table_df = fact_table_df.withColumn(\"fact_order_key\", surrogateUDF(array(\"orderNumber\", \"order_line_number\")))\\\n",
    ".withColumn(\"customer_key\", surrogateUDF(array(\"customer_number\")))\\\n",
    ".withColumn(\"employee_key\", surrogateUDF(array(\"employee_number\")))\\\n",
    ".withColumn(\"office_key\", surrogateUDF(array(\"officeCode\")))\\\n",
    ".withColumn(\"product_key\", surrogateUDF(array(\"productCode\")))\\\n",
    ".select([\"fact_order_key\",\"customer_key\",\"employee_key\",\"office_key\",\"product_key\",\"order_date\",\"order_required_date\",\"order_shipped_date\",\"quantity_ordered\",\"product_price\",\"profit\",\"discount_percentage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.3. Store the result in the `fact_orders` table in your database `classicmodels_star_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:24:05.145356Z",
     "iopub.status.busy": "2025-03-17T10:24:05.145087Z",
     "iopub.status.idle": "2025-03-17T10:24:18.476130Z",
     "shell.execute_reply": "2025-03-17T10:24:18.475508Z",
     "shell.execute_reply.started": "2025-03-17T10:24:05.145329Z"
    },
    "exercise": [
     "ex23"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fe8e9dd48c4f48b0673236f6884b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fact_table_df.write.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.fact_orders\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.4. Check that your table was correctly stored in the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:24:23.819607Z",
     "iopub.status.busy": "2025-03-17T10:24:23.819383Z",
     "iopub.status.idle": "2025-03-17T10:24:24.573988Z",
     "shell.execute_reply": "2025-03-17T10:24:24.573314Z",
     "shell.execute_reply.started": "2025-03-17T10:24:23.819581Z"
    },
    "exercise": [
     "ex24"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbad49c3a404dde84147f9edc740224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_orders column names:  ['fact_order_key', 'customer_key', 'employee_key', 'office_key', 'product_key', 'order_date', 'order_required_date', 'order_shipped_date', 'quantity_ordered', 'product_price', 'profit', 'discount_percentage']\n",
      "fact_orders number of rows:  2996"
     ]
    }
   ],
   "source": [
    "fact_table_df_check = spark.read.jdbc(url=jdbc_url, table=\"classicmodels_star_schema.fact_orders\", properties=jdbc_properties)\n",
    "\n",
    "print(\"fact_orders column names: \", fact_table_df_check.columns)\n",
    "\n",
    "fact_table_row_count = fact_table_df_check.count()\n",
    "print(\"fact_orders number of rows: \", fact_table_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "fact_orders column names:  ['fact_order_key', 'customer_key', 'employee_key', 'office_key', 'product_key', 'order_date', 'order_required_date', 'order_shipped_date', 'quantity_ordered', 'product_price', 'profit', 'discount_percentage']\n",
    "fact_orders number of rows:  2996\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8.5. Finally, print out your schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T10:24:28.060909Z",
     "iopub.status.busy": "2025-03-17T10:24:28.060685Z",
     "iopub.status.idle": "2025-03-17T10:24:28.116359Z",
     "shell.execute_reply": "2025-03-17T10:24:28.115636Z",
     "shell.execute_reply.started": "2025-03-17T10:24:28.060883Z"
    },
    "exercise": [
     "ex25"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc555e0b0ace412c971297da22eb01d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fact_order_key: string (nullable = true)\n",
      " |-- customer_key: string (nullable = true)\n",
      " |-- employee_key: string (nullable = true)\n",
      " |-- office_key: string (nullable = true)\n",
      " |-- product_key: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_required_date: timestamp (nullable = true)\n",
      " |-- order_shipped_date: timestamp (nullable = true)\n",
      " |-- quantity_ordered: integer (nullable = true)\n",
      " |-- product_price: decimal(38,18) (nullable = true)\n",
      " |-- profit: decimal(38,17) (nullable = true)\n",
      " |-- discount_percentage: decimal(38,6) (nullable = true)"
     ]
    }
   ],
   "source": [
    "fact_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "root\n",
    " |-- fact_order_key: string (nullable = true)\n",
    " |-- customer_key: string (nullable = true)\n",
    " |-- employee_key: string (nullable = true)\n",
    " |-- office_key: string (nullable = true)\n",
    " |-- product_key: string (nullable = true)\n",
    " |-- order_date: timestamp (nullable = true)\n",
    " |-- order_required_date: timestamp (nullable = true)\n",
    " |-- order_shipped_date: timestamp (nullable = true)\n",
    " |-- quantity_ordered: integer (nullable = true)\n",
    " |-- product_price: decimal(38,18) (nullable = true)\n",
    " |-- profit: decimal(38,17) (nullable = true)\n",
    " |-- discount_percentage: decimal(38,6) (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you have explored basic data transformation using Apache Spark, focusing on the capabilities of their Python API (PySpark) and Spark SQL. These tools are essential for data engineers, offering powerful and efficient methods for manipulating large datasets. Spark offers rich APIs and tools for data transformations, one of them being Spark SQL, which enables querying of structured data with SQL-like syntax. Although this dataset isn't particularly large, the same principles apply to larger data sources due to data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Upload Files for Grading\n",
    "\n",
    "Upload the notebook into S3 bucket for grading purposes.\n",
    "\n",
    "*Note*: you may need to click **Save** button before the upload.\n",
    "\n",
    "In your AWS console, search again for **CloudShell** and click on it. Once the terminal is ready, execute the following two commands to upload your notebook:\n",
    "\n",
    "```bash\n",
    "export ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "aws s3 cp s3://de-c4w3a1-$ACCOUNT_ID-us-east-1-emr-bucket/emr-studio/$(aws s3 ls s3://de-c4w3a1-$ACCOUNT_ID-us-east-1-emr-bucket/emr-studio --recursive | grep -o \"e-[^/]*\" | head -n 1)/C4_W3_Assignment.ipynb s3://de-c4w3a1-$ACCOUNT_ID-us-east-1-submission/C4_W3_Assignment_Learner.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
